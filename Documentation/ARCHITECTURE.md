# Neuralux Architecture

## Current Implementation Status

This document describes the current state of the Neuralux AI Layer implementation.

## What's Been Built (Phase 1 Foundation)

### âœ… Core Infrastructure

#### Message Bus (NATS)
- **Location**: `docker-compose.yml`
- **Features**:
  - JetStream for message persistence
  - Request/Reply for synchronous operations
  - Pub/Sub for event distribution
  - Health monitoring on port 8222

#### Vector Store (Qdrant)
- **Location**: `docker-compose.yml`
- **Features**:
  - Embedded mode for local vector storage
  - HTTP API (port 6333) and gRPC (port 6334)
  - Ready for semantic search implementation

#### Cache Layer (Redis)
- **Location**: `docker-compose.yml`
- **Features**:
  - LRU eviction policy
  - 1GB memory limit
  - Ready for model caching and session storage

### âœ… Common Package

**Location**: `packages/common/neuralux/`

#### Configuration Management (`config.py`)
- Pydantic-based configuration
- Environment variable support
- XDG directory compliance (`.config`, `.cache`, `.local/share`)
- Hardware profile support (battery_saver, balanced, performance)

#### Message Bus Client (`messaging.py`)
- Async NATS wrapper
- Request/Reply pattern
- Pub/Sub pattern
- Automatic reconnection
- Structured logging

#### Logging (`logger.py`)
- Structured logging with `structlog`
- JSON and console output modes
- Context-aware logging
- Service-specific loggers

### âœ… LLM Service

**Location**: `services/llm/`

#### Backend (`llm_backend.py`)
- llama.cpp integration
- GPU acceleration support
- Configurable model parameters
- Streaming generation
- Embeddings generation

#### REST API (`service.py`)
- FastAPI-based HTTP API
- OpenAI-compatible endpoints:
  - `POST /v1/chat/completions`
  - `POST /v1/embeddings`
  - `GET /v1/models`
- Streaming support
- Message bus integration

#### Features
- Automatic model loading
- Graceful fallback to CPU
- Memory-efficient quantization support
- Context-aware completions

### âœ… CLI Tool (aish)

**Location**: `packages/cli/aish/`

#### Features
- Interactive mode with rich UI
- Single-shot command mode
- Natural language to shell command translation
- Command explanation (`/explain`)
- Execution preview and confirmation
- Context awareness (cwd, git status)
- Service status checking
- Web search with summaries (`/web <query>`), approval to open URLs
- Natural chat mode (`/mode chat`) for conversational answers

#### User Experience
- Syntax highlighting for commands
- Markdown rendering for explanations
- Colored output
- Progress indicators
- Safety confirmations

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Layer                          â”‚
â”‚                   aish CLI tool                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Message Bus (NATS)                      â”‚
â”‚            Request/Reply + Pub/Sub                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Service     â”‚    â”‚  Vision Service    â”‚
â”‚   (llama.cpp)     â”‚    â”‚  (OCR)             â”‚
â”‚   Port: 8000      â”‚    â”‚  Port: 8005        â”‚
â”‚                   â”‚    â”‚  NATS: ai.vision.* â”‚
â”‚                   â”‚    â”‚                    â”‚
â”‚                   â”‚    â”‚  Audio Service     â”‚
â”‚                   â”‚    â”‚  Port: 8006        â”‚
â”‚                   â”‚    â”‚  - Filesystem      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Storage & Cache Layer                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Qdrant  â”‚  â”‚  Redis  â”‚  â”‚  Local  â”‚      â”‚
â”‚  â”‚ Vector  â”‚  â”‚  Cache  â”‚  â”‚  Files  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Communication

### Message Bus Subjects

Implemented:
- `ai.llm.request` - LLM completion requests
- `ai.llm.embed` - Embedding generation
- `ai.audio.stt` - Speech-to-text transcription
- `ai.audio.tts` - Text-to-speech synthesis
- `ai.audio.vad` - Voice activity detection
- `ai.audio.info` - Audio service information
- `system.health.summary` - Current health snapshot and alerts
- `ai.vision.ocr.request` - OCR request/reply
- `ai.vision.ocr.result` - OCR result broadcast
- `ai.vision.imagegen.request` - Image generation request/reply
- `ai.vision.imagegen.model_info` - Get available models and info
- `ai.llm.reload.events` - Model reload lifecycle events (start/done/error)
- `ai.audio.reload.events` - STT model switch lifecycle events

Notes:
- Health summary includes NVIDIA GPU metrics (utilization %, VRAM, temperature, power) when NVML is available.
- `system.file.search` - Semantic search requests
- `ui.overlay.toggle` - Toggle overlay visibility
- `ui.overlay.show` - Show/focus overlay
- `ui.overlay.hide` - Hide overlay

Planned (from plan.md):
- `ai.llm.stream` - Streaming completions
- `ai.vision.*` - Vision service operations
- `system.file.*` - File system operations
- `system.process.*` - Process management
- `user.interaction.*` - User events
- `temporal.snapshot.*` - Temporal intelligence

### Data Flow Example: CLI Command

```
User Input: "show me large files"
        â”‚
        â–¼
    aish CLI
        â”‚ (formats request)
        â–¼
    NATS Request
    (ai.llm.request)
        â”‚
        â–¼
    LLM Service
        â”‚ (generates command)
        â–¼
    NATS Reply
        â”‚
        â–¼
    aish CLI
        â”‚ (displays & confirms)
        â–¼
    Shell Execution
```

## File Organization

```
NeuroTuxLayer/
â”œâ”€â”€ docker-compose.yml          # Infrastructure services
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Makefile                    # Build & run commands
â”œâ”€â”€ QUICKSTART.md              # Getting started guide
â”œâ”€â”€ plan.md                     # Original project plan
â”œâ”€â”€ ARCHITECTURE.md            # This file
â”‚
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ common/                # Shared utilities
â”‚   â”‚   â””â”€â”€ neuralux/
â”‚   â”‚       â”œâ”€â”€ config.py      # Configuration
â”‚   â”‚       â”œâ”€â”€ messaging.py   # Message bus client
â”‚   â”‚       â””â”€â”€ logger.py      # Logging setup
â”‚   â”‚
â”‚   â””â”€â”€ cli/                   # Command line interface
â”‚       â””â”€â”€ aish/
â”‚           â””â”€â”€ main.py        # CLI implementation
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm/                   # Language model service
â”‚   â”‚   â”œâ”€â”€ config.py          # Service config
â”‚   â”‚   â”œâ”€â”€ models.py          # Data models
â”‚   â”‚   â”œâ”€â”€ llm_backend.py     # llama.cpp backend
â”‚   â”‚   â””â”€â”€ service.py         # FastAPI service
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                 # Audio service (Phase 2B)
â”‚   â”‚   â”œâ”€â”€ config.py          # Service config
â”‚   â”‚   â”œâ”€â”€ models.py          # Data models
â”‚   â”‚   â”œâ”€â”€ stt_backend.py     # faster-whisper STT
â”‚   â”‚   â”œâ”€â”€ tts_backend.py     # Piper TTS
â”‚   â”‚   â”œâ”€â”€ vad_backend.py     # Silero VAD
â”‚   â”‚   â””â”€â”€ service.py         # FastAPI service
â”‚   â”‚
â”‚   â”œâ”€â”€ vision/                # Vision service (Phase 2B)
â”‚   â”‚   â”œâ”€â”€ config.py          # Service config
â”‚   â”‚   â”œâ”€â”€ models.py          # Data models
â”‚   â”‚   â”œâ”€â”€ ocr_backend.py     # PaddleOCR backend
â”‚   â”‚   â”œâ”€â”€ image_gen_backend.py # Flux/SDXL image generation
â”‚   â”‚   â””â”€â”€ service.py         # FastAPI service
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start-services.sh      # Start all services
â”‚   â””â”€â”€ stop-services.sh       # Stop all services
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_config.py         # Config tests
    â””â”€â”€ test_message_bus.py    # Message bus tests
```

## Next Steps (From Plan)

### Phase 2A âœ… COMPLETE
- [x] System health monitoring
- [x] GUI overlay assistant
- [x] System tray integration and desktop packaging

### Phase 2B (Intelligence) ğŸš§ In Progress
- [x] Voice interface (STT/TTS) âœ… **IMPLEMENTED**
  - Speech-to-text with faster-whisper
  - Text-to-speech with Piper
  - Voice activity detection with Silero VAD
  - CLI commands: `aish listen`, `aish speak`
- [x] Vision service (OCR + Image Generation) âœ… **IMPLEMENTED**
  - OCR with PaddleOCR
    - REST: POST /v1/ocr
    - NATS: ai.vision.ocr.request / ai.vision.ocr.result
    - Overlay: OCR active window, Quick Actions (Copy, Summarize, Translate, Extract), Continue chat/Start fresh
    - Shared session memory via Redis (24h TTL)
  - Image Generation with Flux AI
    - Models: FLUX.1-schnell (4-step fast), FLUX.1-dev (high quality), SDXL-Lightning
    - 8-bit quantization with bitsandbytes for VRAM efficiency
    - REST: POST /v1/generate-image, GET /v1/model-info, GET /v1/progress-stream (SSE)
    - Overlay: ğŸ¨ button for inline generation, preview, Save/Copy/Continue chat
    - Settings: Model, size (512-1024px), steps configurable via tray â†’ Settings
- [ ] Temporal intelligence system
 - [x] Overlay enhancements
   - Settings window (LLM/STT), About dialog, tray menu updates
   - OCR region select (mouse), toast notifications, busy spinner
   - Conversation history + restore; slash command palette
   - Web search integration and fixes

### Phase 3 (Advanced Features)
- [ ] Gesture recognition
- [ ] Developer tool integration
- [x] Media generation (Image generation with Flux AI) âœ…
- [ ] Collaboration features
- [ ] Plugin system

### Phase 4 (Optimization)
- [ ] Performance optimization
- [ ] Multi-GPU support
- [ ] Distributed processing
- [ ] Production hardening

## Technology Choices

### Why NATS?
- Lightweight and fast
- Built-in persistence (JetStream)
- Multiple communication patterns
- Easy to scale

### Why llama.cpp?
- Broad model compatibility
- Efficient quantization
- GPU and CPU support
- No heavy frameworks required

### Why FastAPI?
- Modern async Python
- Automatic API documentation
- Type safety with Pydantic
- High performance

### Why Qdrant?
- Embedded mode for simplicity
- Production-ready
- Advanced filtering
- Good Python support

## Development Workflow

1. **Start Infrastructure**: `make start`
2. **Install Packages**: `make install`
3. **Run LLM Service**: `cd services/llm && python service.py`
4. **Use CLI**: `aish`

## Configuration

Configuration is environment-based with sensible defaults:

- **Development**: Uses `localhost` for all services
- **Production**: Override via environment variables
- **Custom**: Create `.env` file

Key configuration points:
- GPU acceleration: `LLM_GPU_LAYERS`
- Model selection: `LLM_DEFAULT_MODEL`
- Resource limits: `MAX_VRAM_GB`
- Privacy: `TELEMETRY_ENABLED`, `CLOUD_OFFLOAD_ENABLED`

## Testing

```bash
# Run all tests
make test

# Run specific test
pytest tests/test_config.py -v
```

Note: Message bus tests require running infrastructure.

## Logging

All services use structured logging:
- **Development**: Pretty console output
- **Production**: JSON for log aggregation
- **Levels**: DEBUG, INFO, WARNING, ERROR

View logs:
```bash
# Infrastructure logs
docker compose logs -f

# Service logs
# (check terminal where service is running)
```

## Performance Considerations

### Current State
- **Model Loading**: ~5-10 seconds for 3B model
- **Inference**: ~10-50 tokens/second (GPU dependent)
- **Memory**: ~2-4GB VRAM for 4-bit quantized models

### Future Optimizations
- Model caching for faster restarts
- Batched inference for multiple requests
- KV cache sharing
- TensorRT optimization

## Security

### Implemented
- No telemetry by default
- Local-first processing
- Execution confirmation in CLI

### Planned
- AppArmor/SELinux profiles
- Capability-based permissions
- Action audit logging
- Sandboxed plugin execution

## Contributing

The foundation is now in place. Key areas for contribution:

1. **Additional Services**: Implement vision, audio, filesystem services
2. **UI Components**: Build the GUI overlay
3. **Models**: Add support for more model types
4. **Testing**: Expand test coverage
5. **Documentation**: Improve user guides

See the original `plan.md` for the complete vision.

